{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computación Avanzada y sus Aplicaciones a Ingeniería\n",
    "\n",
    "### Máster Universitario en Ingeniería Informática\n",
    "\n",
    "\n",
    "# Práctica 3 - Parte I - Machine Learning con Spark\n",
    "\n",
    "En esta práctica veremos cómo utilizar las técnicas de Machine Learning disponibles en la librería Mlib de Apache Spark. Sigue detenidamente todos los bloques y prueba a cambiar los valores establecidos para comprobar su funcionamiento.\n",
    "\n",
    "Ten en cuenta que una vez tengas en marcha Spark, podrás visualizar la evolución de cada trabajo de Spark en  <http://localhost:4040>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de estar utilizando pySpark, **NO** es necesario inicializar el `SparkSession`, es decir, **no** ejecutar la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Ejemplo pySparkSQL\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///D:/tmp/spark-warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de estar usando pySpark, ejecutar el siguiente comando o inciiar pyspark con \n",
    "\n",
    "`pyspark --conf spark.sql.warehouse.dir=file:///D:/tmp/spark-warehouse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.warehouse.dir\", \"file:///D:/tmp/spark-warehouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otros imports necesarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from test_helper import Test\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos de código con LogisticRegression\n",
    "Vamos a entrenar un modelo de regresión logística con MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.01)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values >= 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class' threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creamos unos datos de entrenamiento como una lista de tuplas (label, features).\n",
    "training = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# Creamos una instancia de LogisticRegression. Esta instancia es un Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "# Imprimimos los parámetros, documentación y valores por defecto.\n",
    "print \"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\"\n",
    "\n",
    "# Aprendemos un modelo LogisticRegression. Utiliza los parámetros almacenados en lr.\n",
    "model1 = lr.fit(training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otra forma de especificación de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# También podemos especificar los parámetros usando un dictionary de Python como paramMap\n",
    "paramMap = {lr.maxIter: 20}\n",
    "paramMap[lr.maxIter] = 30  # Especificamos 1 Param, sobrescribiendo el maxIter original.\n",
    "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # Especificamos múltiples parámetros\n",
    "\n",
    "# Podemos combinar paramMaps, son diccionarios de Python\n",
    "paramMap2 = {lr.probabilityCol: \"myProbability\"}  # Cambiamos el nombre de la columna de salida\n",
    "paramMapCombined = paramMap.copy()\n",
    "paramMapCombined.update(paramMap2)\n",
    "\n",
    "# Aprendemos un nuevo modelo usando los parámetros en paramMapCombined.\n",
    "# paramMapCombined sobrescribe todos los parámetros establecidos anteriormente con lr.set*\n",
    "model2 = lr.fit(training, paramMapCombined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtenemos los resultados en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(features=DenseVector([-1.0, 1.5, 1.3]), label=1.0, myProbability=DenseVector([0.0571, 0.9429]), prediction=1.0)\n",
      "Row(features=DenseVector([3.0, 2.0, -0.1]), label=0.0, myProbability=DenseVector([0.9239, 0.0761]), prediction=0.0)\n",
      "Row(features=DenseVector([0.0, 2.2, -1.5]), label=1.0, myProbability=DenseVector([0.1097, 0.8903]), prediction=1.0)\n"
     ]
    }
   ],
   "source": [
    "# Preparamos el conjunto de test\n",
    "test = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# Obtenemos las predicciones sobre los datos de test usando Transformer.transform()\n",
    "# LogisticRegression.transform solo usa la columna llamada features\n",
    "# El método model2.transform() devuelve una columna \"myProbability\" column en vez de \n",
    "# 'probability' ya que hemos cambiado el parámetro lr.probabilityCol \n",
    "prediction = model2.transform(test)\n",
    "selected = prediction.select(\"features\", \"label\", \"myProbability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos de código con LogisticRegression: Pipeline\n",
    "Vamos a entrenar un modelo de regresión logística con MLlib. En este caso, haremos todo el proceso mediante una pipeline, incluyendo la creación de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Preparamos los documentos de entrenamiento a partir de una lista de tuplas (id, text, label)\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configuramos una pipeline de ML, que consiste en tres etapas: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Entrenamos la pipeline con los documentos\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtenemos los resultados en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text=u'spark i j k', prediction=0.0)\n",
      "Row(id=5, text=u'l m n', prediction=0.0)\n",
      "Row(id=6, text=u'mapreduce spark', prediction=0.0)\n",
      "Row(id=7, text=u'apache hadoop', prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Preparamos el conjunto de test con documentos no etiquetados (id, text)\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")], [\"id\", \"text\"])\n",
    "\n",
    "# Hacemos las predicciones sobre el test\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selección de modelos / ajuste de parámetros\n",
    "Spark permite llevar acabo la selección de parámetros de los algoritmos mediante dos formas:\n",
    "* CrossValidation\n",
    "* Train/Validation split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de modelos con CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Preparamos los documentos de entrenamiento etiquetados.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configuramos la pipeline, que consiste de tres etapas:  tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossValidator requiere de un estimador, un conjunto de parámetros sobre los que realizar la búsqueda y un evaluador. Además, hay que indicar el número de particiones a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tratamos la Pipeline como un Estimator para incluirla en una instancia de CrossValidator.\n",
    "# De esta forma podemos optimizar los parámetros de todas las fases de manera conjunta.\n",
    "# Un CrossValidator require de un Estimator, un conjunto de ParamMaps del Estimator y un Evaluator.\n",
    "# Hacemos usod del ParamGridBuilder para construir el grid de parámetros.\n",
    "# Usamos 3 valores para hashingTF.numFeatures y 2 valores para lr.regParam,\n",
    "# Este grid tendrá 3 x 2 = 6 combinaciones de las que se elegirá el modelo.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # usar más de 3 particiones en la práctica\n",
    "\n",
    "# Ejecutamos la validación cruzada, y elegimos el mejor conjunto de parámetros.\n",
    "cvModel = crossval.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparamos los documentos de test sin etiqueta\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Realizamos las predicciones sobre los documentos de test. cvModel  utiliza el mejor modelo encontrado.\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"prediction\", \"probability\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de modelos con TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "# Preparamos los datos de entrenamiento y test.\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"sample_linear_regression_data.txt\")\n",
    "train, test = data.randomSplit([0.7, 0.3])\n",
    "lr = LinearRegression(maxIter=10, regParam=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrainValidationSplit requiere de un estimador, un conjunto de parámetros sobre los que realizar la búsqueda y un evaluador. Además, hay que indicar el tamaño del training respecto a la validación (trainRatio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Usamos un ParamGridBuilder para construir el grid de parámetros sobre el que realizar la búsqueda.\n",
    "# TrainValidationSplit probará todas las combinaciones de valores para quedarse con la mejor\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "# El estimador es la regresión lineal.\n",
    "# Un TrainValidationSplit requiere de un Estimator, un conjunto de parámetros ParamMaps y un Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "\n",
    "# Ejecutamos TrainValidationSplit, y obtenemos la mejor combinación de parámetros.\n",
    "model = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hacemos las predicciones en test. model es el modelo con la mejor combinación de parámetros.\n",
    "prediction = model.transform(test)\n",
    "for row in prediction.take(5):\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
